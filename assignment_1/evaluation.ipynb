{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhishek/doctr_env/lib/python3.10/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.16) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\n",
      "/home/abhishek/doctr_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset csv (/home/abhishek/.cache/huggingface/datasets/VLyb___csv/VLyb--WN18RR-4fa056f5bc0c9d9e/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
      "100%|██████████| 3/3 [00:00<00:00, 450.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerEncoderModel(\n",
       "  (embedding): Embedding(\n",
       "    (tok_embed): Embedding(40772, 128)\n",
       "    (pos_embed): Embedding(7, 128)\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=128, out_features=40772, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# def set_seed(seed: int = 42) -> None:\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     # When running on the CuDNN backend, two further options must be set\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "#     # Set a fixed value for the hash seed\n",
    "#     os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "#     print(f\"Random seed set as {seed}\")\n",
    "# set_seed()\n",
    "\n",
    "# import math\n",
    "sub_dict = defaultdict(int)\n",
    "\n",
    "##### for Wn data\n",
    "x = load_dataset('VLyb/WN18RR')\n",
    "\n",
    "train_data = []\n",
    "for i in x['train']:\n",
    "    train_data.append([i['head'], i['relation'], i['tail']])\n",
    "    sub_dict[i['head']] += 1\n",
    "    sub_dict[i['relation']] += 1\n",
    "    sub_dict[i['tail']] += 1\n",
    "\n",
    "val_data = []\n",
    "for i in x['validation']:\n",
    "    val_data.append([i['head'], i['relation'], i['tail']])\n",
    "    sub_dict[i['head']] += 1\n",
    "    sub_dict[i['relation']] += 1\n",
    "    sub_dict[i['tail']] += 1\n",
    "\n",
    "test_data = []\n",
    "for i in x['validation']:\n",
    "    test_data.append([i['head'], i['relation'], i['tail']])\n",
    "    sub_dict[i['head']] += 1\n",
    "    sub_dict[i['relation']] += 1\n",
    "    sub_dict[i['tail']] += 1\n",
    "\n",
    "###### for fb15k data\n",
    "# data = open('./datasets/fb15k/train.txt', 'r').read().split('\\n')\n",
    "# data_dev = open('./datasets/fb15k/valid.txt', 'r').read().split('\\n')\n",
    "# data_test = open('./datasets/fb15k/test.txt', 'r').read().split('\\n')\n",
    "# train_data = []\n",
    "# val_data = []\n",
    "# test_data = []\n",
    "# from collections import defaultdict\n",
    "# sub_dict = defaultdict(int)\n",
    "# for i in data[:-1]:\n",
    "#     train_data.append(i.split(\"\\t\"))\n",
    "#     sub_dict[i.split(\"\\t\")[0]] += 1\n",
    "#     sub_dict[i.split(\"\\t\")[1]] += 1\n",
    "#     sub_dict[i.split(\"\\t\")[2]] += 1\n",
    "\n",
    "# for i in data_dev[:-1]:\n",
    "#     dev_data.append(i.split(\"\\t\"))\n",
    "#     sub_dict[i.split(\"\\t\")[0]] += 1\n",
    "#     sub_dict[i.split(\"\\t\")[1]] += 1\n",
    "#     sub_dict[i.split(\"\\t\")[2]] += 1\n",
    "\n",
    "# for i in data_test[:-1]:\n",
    "#     test_data.append(i.split(\"\\t\"))\n",
    "#     sub_dict[i.split(\"\\t\")[0]] += 1\n",
    "#     sub_dict[i.split(\"\\t\")[1]] += 1\n",
    "#     sub_dict[i.split(\"\\t\")[2]] += 1\n",
    "\n",
    "sub = {j : i + 4 for i, j in enumerate(sub_dict)}\n",
    "# target_dict = {j : i + 4 for i, j in enumerate(target_dict)}\n",
    "sub['[CLS]'] = 0\n",
    "sub['[SEP]'] = 1\n",
    "sub['[END]'] = 2\n",
    "sub['[MASK]'] = 3\n",
    "\n",
    "vocab_size = len(sub)\n",
    "# target_size = len(target_dict)\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "hidden_dim = 512\n",
    "num_layers = 4\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "max_len = 7\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# %%\n",
    "class Embedding(nn.Module):\n",
    "   def __init__(self):\n",
    "       super(Embedding, self).__init__()\n",
    "       self.tok_embed = nn.Embedding(vocab_size, d_model)\n",
    "       self.pos_embed = nn.Embedding(max_len, d_model)\n",
    "       self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "   def forward(self, x):\n",
    "       seq_len = x.size(1)\n",
    "       pos = torch.arange(seq_len, dtype=torch.long).to(device)\n",
    "       pos = pos.unsqueeze(0).expand_as(x)\n",
    "       embedding = self.tok_embed(x) + self.pos_embed(pos)\n",
    "       return self.norm(embedding)\n",
    "\n",
    "# %%\n",
    "class TransformerEncoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers,dropout: float = 0.5):\n",
    "        super(TransformerEncoderModel, self).__init__()\n",
    "        self.embedding = Embedding()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers, mask_check=False)\n",
    "\n",
    "        self.classifier = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        src = self.embedding(input_ids)\n",
    "        transformer_output = self.transformer_encoder(src)\n",
    "        output_logits = self.classifier(transformer_output[:, -2, :])\n",
    "        return output_logits\n",
    "# %%\n",
    "class MaskedGenerationDataset(Dataset):\n",
    "    def __init__(self, train_data):\n",
    "        self.data = train_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        triplet = self.data[idx]\n",
    "        subject_id, relation_id, object_id = triplet[0], triplet[1], triplet[2]\n",
    "\n",
    "        input_ids_masked = torch.tensor([0, sub[subject_id], 1, sub[relation_id], 1, 3, 2], dtype=torch.long)\n",
    "        tgt = torch.tensor(sub[object_id])\n",
    "        return input_ids_masked, tgt\n",
    "\n",
    "# %%\n",
    "# Create an instance of the custom dataset\n",
    "dataset_train = MaskedGenerationDataset(train_data)\n",
    "dataset_val = MaskedGenerationDataset(val_data)\n",
    "dataset_test = MaskedGenerationDataset(test_data)\n",
    "\n",
    "# Create a DataLoader for masked generation approach\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = TransformerEncoderModel(vocab_size, d_model, num_heads, hidden_dim, num_layers)\n",
    "model.load_state_dict(torch.load('transformer_model_wn.pth'))\n",
    "model.load_state_dict(torch.load('transformer_model_fn.pth'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "    for test_input_ids, test_target_ids in test_dataloader:\n",
    "        output_logits = model(test_input_ids.to(device))\n",
    "        outputs.append(output_logits)\n",
    "        targets.append(test_target_ids)\n",
    "\n",
    "outputs = torch.cat(outputs, dim=0).to(device)\n",
    "targets = torch.cat(targets, dim=0).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "total_hits = 0\n",
    "total_hits1 = 0\n",
    "total_mrr = 0\n",
    "total_map = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_input_ids, test_target_ids in test_dataloader:\n",
    "        output_logits = model(test_input_ids.to(device))\n",
    "        _, indices = torch.topk(output_logits, 1, dim=1)\n",
    "        \n",
    "        # Calculate Hits@1\n",
    "        hits1 = torch.sum(indices == test_target_ids.unsqueeze(1).to(device), dim=1)\n",
    "        total_hits1 += torch.sum(hits > 0).item()\n",
    "\n",
    "        # Calculate Hits@10\n",
    "        hits = torch.sum(indices == test_target_ids.unsqueeze(1).to(device), dim=1)\n",
    "        total_hits += torch.sum(hits > 0).item()\n",
    "\n",
    "        # Calculate MRR\n",
    "        reciprocal_ranks = torch.zeros_like(test_target_ids, dtype=torch.float)\n",
    "        for i in range(len(test_target_ids)):\n",
    "            rank = torch.where(indices[i] == test_target_ids[i])[0]\n",
    "            if len(rank) > 0:\n",
    "                reciprocal_ranks[i] = 1.0 / (rank[0].item() + 1)\n",
    "        total_mrr += torch.sum(reciprocal_ranks).item()\n",
    "\n",
    "        # Calculate MAP\n",
    "        avg_precision = torch.zeros_like(test_target_ids, dtype=torch.float)\n",
    "        for i in range(len(test_target_ids)):\n",
    "            num_hits = 0\n",
    "            precision_at_i = 0\n",
    "            for j, index in enumerate(indices[i]):\n",
    "                if index == test_target_ids[i]:\n",
    "                    num_hits += 1\n",
    "                    precision_at_i += num_hits / (j + 1)\n",
    "            if num_hits > 0:\n",
    "                avg_precision[i] = precision_at_i / num_hits\n",
    "        total_map += torch.sum(avg_precision).item()\n",
    "\n",
    "# Calculate final scores\n",
    "total_samples = len(test_dataloader.dataset)\n",
    "hits_at_10 = total_hits / total_samples\n",
    "hits_at_1 = total_hits1 / total_samples\n",
    "mrr = total_mrr / total_samples\n",
    "map_score = total_map / total_samples\n",
    "\n",
    "hits_at_1, hits_at_10, mrr, map_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doctr_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
